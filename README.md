# AI-at-Scale
This repo contains code and report of semester project titled **Distilling knowledge from BERT into Simpler Machine Learning Models**

# Problem Statement 
As a core task of natural language processing and information retrieval, performance of algorithms plays a vital role. This survey paper draws comparison between various distillation models, which generates predictions from the whole ensemble of models less computationally expensive. We propose a simple compression pipeline which achieves considerable amount of reduction in model size without distorting accuracy. We also explore methods of interpretability of complex models as a future line of work.

# Collaborators
- Praveen Sridhar
- Rakesh Varma Siri
- Rashmi Nagpal
- Ribhu Lahiri



